## 6.4 Decision tree learning algorithm

<a href="https://www.youtube.com/watch?v=XODz6LwKY7g&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR"><img src="images/thumbnail-6-04.jpg"></a>

[Slides](https://www.slideshare.net/AlexeyGrigorev/ml-zoomcamp-6-decision-trees-and-ensemble-learning)


## Notes

This lesson first reviews the topics learned in previous lesson about training a decision tress using sklearn, how to handle the model not generalizing due to overfitting of the data. 

In this lesson, we learn about how to best split a decision tree and different classification criteria that can be used to split a tree. We dive deep using an example and splitting the tree using `misclassification` criteria. Additionally, different stopping criteria to break the iterative tree split criteria are discussed.     

Add notes from the video (PRs are welcome)

* structure of a decision tree: nodes & leaves
* depth of a decision tree & levels
* rules & conditions, thresholds
* misclassification rate
* impurity criteria (i.e. MSE)
* decision trees can be used to solve regression problems

<table>
   <tr>
      <td>⚠️</td>
      <td>
         The notes are written by the community. <br>
         If you see an error here, please create a PR with a fix.
      </td>
   </tr>
</table>


## Navigation

* [Machine Learning Zoomcamp course](../)
* [Session 6: Decision Trees and Ensemble Learning](./)
* Previous: [Decision trees](03-decision-trees.md)
* Next: [Decision trees parameter tuning](05-decision-tree-tuning.md)
